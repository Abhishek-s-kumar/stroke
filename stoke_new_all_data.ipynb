{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNaGxRHYolcbEfWc+d5ORuJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishek-s-kumar/stroke/blob/main/stoke_new_all_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# SECTION 1: INSTALLATION AND SETUP\n",
        "# ==========================================\n",
        "\n",
        "# Install required packages\n",
        "!pip -q install kaggle timm einops albumentations==1.4.6 torchmetrics wandb kagglehub grad-cam pandas scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "import kagglehub\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc,\n",
        "                           precision_recall_curve, average_precision_score,\n",
        "                           classification_report, accuracy_score)\n",
        "\n",
        "print(\"âœ… All packages imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akU_v8MZkbPq",
        "outputId": "296efac6-2d7a-498f-82c9-0ecaf24be9bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All packages imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 2: DATASET DOWNLOAD AND SETUP\n",
        "# ==========================================\n",
        "\n",
        "# Download dataset using kagglehub\n",
        "print(\"ðŸ“¥ Downloading dataset...\")\n",
        "path = kagglehub.dataset_download(\"turkertuncer/multimodal-stroke-image-dataset\")\n",
        "print(\"âœ… Dataset downloaded to:\", path)\n",
        "\n",
        "# Update DATA_DIR to use the downloaded path\n",
        "DATA_DIR = os.path.join(path, \"deep\")\n",
        "\n",
        "# Verify dataset structure\n",
        "print(\"\\nðŸ“ Dataset structure:\")\n",
        "for root, dirs, files in os.walk(DATA_DIR):\n",
        "    level = root.replace(DATA_DIR, \"\").count(os.sep)\n",
        "    indent = \" \" * 2 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    for d in dirs:\n",
        "        print(f\"{indent}  {d}/\")\n",
        "    if level > 2:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT_O_4IFkgW4",
        "outputId": "2a370b6a-77b8-4226-d6ee-6de070c30e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¥ Downloading dataset...\n",
            "âœ… Dataset downloaded to: /kaggle/input/multimodal-stroke-image-dataset\n",
            "\n",
            "ðŸ“ Dataset structure:\n",
            "deep/\n",
            "  test/\n",
            "  train/\n",
            "  test/\n",
            "    strokeMR/\n",
            "    normalBT/\n",
            "    strokeBT/\n",
            "    normalMR/\n",
            "    strokeMR/\n",
            "    normalBT/\n",
            "    strokeBT/\n",
            "    normalMR/\n",
            "  train/\n",
            "    2- Control/\n",
            "    1- Stroke/\n",
            "    2- Control/\n",
            "      NormalBT/\n",
            "      NormalMR/\n",
            "      NormalBT/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# SECTION 3: DATASET CLASS AND UTILITIES\n",
        "# ==========================================\n",
        "\n",
        "def gather_images(folders):\n",
        "    \"\"\"Gather all image paths from given folders\"\"\"\n",
        "    paths = []\n",
        "    for folder in folders:\n",
        "        if os.path.exists(folder):\n",
        "            for ext in ('*.png', '*.jpg', '*.jpeg'):\n",
        "                paths.extend(glob.glob(os.path.join(folder, '**', ext), recursive=True))\n",
        "    return paths\n",
        "\n",
        "class CustomStrokeDataset(Dataset):\n",
        "    def __init__(self, folders_dict, transform):\n",
        "        self.paths = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "\n",
        "        for label_name, folders in folders_dict.items():\n",
        "            label = 1 if \"stroke\" in label_name.lower() else 0\n",
        "            imgs = gather_images(folders)\n",
        "            self.paths.extend(imgs)\n",
        "            self.labels.extend([label]*len(imgs))\n",
        "            print(f\"âœ… Loaded {len(imgs)} images for class '{label_name}' (label={label})\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return img, label\n",
        "\n",
        "print(\"âœ… Dataset utilities defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmA6Mm-fknDh",
        "outputId": "843f84ae-0385-4006-d45f-d155c77a5906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dataset utilities defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 4: MODEL ARCHITECTURE\n",
        "# ==========================================\n",
        "\n",
        "import timm\n",
        "\n",
        "class ResNet50_ViT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Load pre-trained models\n",
        "        self.resnet = timm.create_model('resnet50', pretrained=True)\n",
        "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "\n",
        "        # Remove original classifiers\n",
        "        self.resnet.global_pool = nn.Identity()\n",
        "        self.resnet.fc = nn.Identity()\n",
        "        self.vit.head = nn.Identity()\n",
        "\n",
        "        # Add custom components\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2048+768, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ResNet features: (B, 2048, H, W) -> (B, 2048)\n",
        "        r_feat = self.pool(self.resnet.forward_features(x)).flatten(1)\n",
        "\n",
        "        # ViT features: (B, N, 768), take CLS token\n",
        "        v_feat = self.vit.forward_features(x)\n",
        "        if v_feat.ndim == 3:  # (B, N, C)\n",
        "            v_feat = v_feat[:, 0, :]  # CLS token\n",
        "\n",
        "        # Concatenate and classify\n",
        "        combined = torch.cat([r_feat, v_feat], dim=1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "print(\"âœ… Model architecture defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qUGvAlqkpL4",
        "outputId": "f8247486-6e76-462a-f37f-927dee45d7cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model architecture defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 5: DATA PREPARATION\n",
        "# ==========================================\n",
        "\n",
        "# Configuration\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Data transforms\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# Dataset folders\n",
        "TRAIN_FOLDERS = {\n",
        "    \"stroke\": [f\"{DATA_DIR}/train/1- Stroke\"],\n",
        "    \"normal\": [f\"{DATA_DIR}/train/2- Control\"]\n",
        "}\n",
        "\n",
        "VAL_FOLDERS = {\n",
        "    \"stroke\": [f\"{DATA_DIR}/test/strokeBT\", f\"{DATA_DIR}/test/strokeMR\"],\n",
        "    \"normal\": [f\"{DATA_DIR}/test/normalBT\", f\"{DATA_DIR}/test/normalMR\"]\n",
        "}\n",
        "\n",
        "# Create datasets\n",
        "print(\"ðŸ“Š Creating datasets...\")\n",
        "train_ds = CustomStrokeDataset(TRAIN_FOLDERS, train_tf)\n",
        "val_ds = CustomStrokeDataset(VAL_FOLDERS, val_tf)\n",
        "\n",
        "# Create data loaders\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"âœ… Train samples: {len(train_ds)} | Val samples: {len(val_ds)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLun1YEjkr7B",
        "outputId": "93ce108f-c49c-4020-e228-76cb92f151d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Š Creating datasets...\n",
            "âœ… Loaded 393 images for class 'stroke' (label=1)\n",
            "âœ… Loaded 1984 images for class 'normal' (label=0)\n",
            "âœ… Loaded 119 images for class 'stroke' (label=1)\n",
            "âœ… Loaded 657 images for class 'normal' (label=0)\n",
            "âœ… Train samples: 2377 | Val samples: 776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 6: MODEL TRAINING\n",
        "# ==========================================\n",
        "\n",
        "# Initialize model and training components\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"ðŸ”§ Using device: {device}\")\n",
        "\n",
        "model = ResNet50_ViT().to(device)\n",
        "\n",
        "from torchmetrics.classification import BinaryAccuracy, AUROC\n",
        "\n",
        "# Training configuration\n",
        "EPOCHS = 35\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "acc_metric = BinaryAccuracy().to(device)\n",
        "auc_metric = AUROC(task='binary').to(device)\n",
        "\n",
        "# Training tracking\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "val_aurocs = []\n",
        "\n",
        "print(\"ðŸš€ Starting training...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_batches = 0\n",
        "\n",
        "    for xb, yb in train_dl:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * xb.size(0)\n",
        "        train_batches += 1\n",
        "\n",
        "    train_loss /= len(train_ds)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    all_val_logits = []\n",
        "    all_val_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_dl:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            val_loss += criterion(logits, yb).item() * xb.size(0)\n",
        "            all_val_logits.append(logits.softmax(1)[:, 1].detach().cpu())\n",
        "            all_val_targets.append(yb.detach().cpu())\n",
        "\n",
        "    val_loss /= len(val_ds)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Calculate metrics\n",
        "    all_val_logits = torch.cat(all_val_logits)\n",
        "    all_val_targets = torch.cat(all_val_targets)\n",
        "\n",
        "    acc = acc_metric(all_val_logits, all_val_targets)\n",
        "    auc_score = auc_metric(all_val_logits, all_val_targets)\n",
        "\n",
        "    val_accuracies.append(acc.item())\n",
        "    val_aurocs.append(auc_score.item())\n",
        "\n",
        "    # Print progress\n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        print(f\"Epoch {epoch+1:2d}/{EPOCHS} - Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Acc: {acc:.4f} | AUROC: {auc_score:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n",
        "\n",
        "print(\"âœ… Training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo0lMgerku85",
        "outputId": "5ef3838a-565c-42e2-e941-dc2b9758eb75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”§ Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Starting training...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 7: TRAINING VISUALIZATION\n",
        "# ==========================================\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(range(1, EPOCHS+1), train_losses, 'b-', label='Train Loss', linewidth=2)\n",
        "plt.plot(range(1, EPOCHS+1), val_losses, 'r-', label='Val Loss', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(range(1, EPOCHS+1), val_accuracies, 'g-', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(range(1, EPOCHS+1), val_aurocs, 'purple', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUROC')\n",
        "plt.title('Validation AUROC')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"ðŸ“Š Final Training Results:\")\n",
        "print(f\"   Best Validation Accuracy: {max(val_accuracies):.4f}\")\n",
        "print(f\"   Best Validation AUROC: {max(val_aurocs):.4f}\")\n"
      ],
      "metadata": {
        "id": "uWb3Rx0nkxqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# SECTION 8: COMPLETE DATASET EVALUATION\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ” EVALUATING ON COMPLETE VALIDATION DATASET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Final evaluation on all validation data\n",
        "model.eval()\n",
        "all_logits = []\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "all_image_paths = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (xb, yb) in enumerate(val_dl):\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        probs = logits.softmax(1)[:, 1]  # Probability of stroke class\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        # Store results\n",
        "        all_logits.extend(probs.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_targets.extend(yb.cpu().numpy())\n",
        "\n",
        "        # Store corresponding image paths\n",
        "        start_idx = batch_idx * BATCH_SIZE\n",
        "        end_idx = min(start_idx + len(xb), len(val_ds))\n",
        "        batch_paths = val_ds.paths[start_idx:end_idx]\n",
        "        all_image_paths.extend(batch_paths)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_logits = np.array(all_logits)\n",
        "all_preds = np.array(all_preds)\n",
        "all_targets = np.array(all_targets)\n",
        "\n",
        "print(f\"âœ… Processed {len(all_targets)} validation images\")\n"
      ],
      "metadata": {
        "id": "3CHnXzYwk0fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# SECTION 9: PERFORMANCE METRICS\n",
        "# ==========================================\n",
        "\n",
        "# Calculate overall performance\n",
        "overall_accuracy = accuracy_score(all_targets, all_preds)\n",
        "\n",
        "print(\"\\nðŸ“ˆ OVERALL PERFORMANCE METRICS:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(f\"\\nðŸ“‹ DETAILED CLASSIFICATION REPORT:\")\n",
        "print(classification_report(all_targets, all_preds, target_names=['Normal', 'Stroke']))\n"
      ],
      "metadata": {
        "id": "39BPxxwMk255"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# SECTION 10: VISUALIZATION - CONFUSION MATRIX\n",
        "# ==========================================\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(all_targets, all_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Normal\", \"Stroke\"])\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "plt.title(f\"Confusion Matrix - Complete Dataset\\n(n={len(all_targets)} images)\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Print confusion matrix values\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"ðŸ“Š Confusion Matrix Breakdown:\")\n",
        "print(f\"   True Negatives (Normal â†’ Normal): {tn}\")\n",
        "print(f\"   False Positives (Normal â†’ Stroke): {fp}\")\n",
        "print(f\"   False Negatives (Stroke â†’ Normal): {fn}\")\n",
        "print(f\"   True Positives (Stroke â†’ Stroke): {tp}\")\n"
      ],
      "metadata": {
        "id": "WKijiI0Ok4-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# SECTION 11: ROC AND PR CURVES\n",
        "# ==========================================\n",
        "\n",
        "# Calculate curves\n",
        "fpr, tpr, roc_thresholds = roc_curve(all_targets, all_logits)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "precision, recall, pr_thresholds = precision_recall_curve(all_targets, all_logits)\n",
        "ap_score = average_precision_score(all_targets, all_logits)\n",
        "\n",
        "# Plot curves\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# ROC Curve\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.6)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Precision-Recall Curve\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(recall, precision, color='purple', lw=2, label=f'PR Curve (AP = {ap_score:.3f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Probability distribution\n",
        "plt.subplot(1, 3, 3)\n",
        "stroke_probs = all_logits[all_targets == 1]\n",
        "normal_probs = all_logits[all_targets == 0]\n",
        "\n",
        "plt.hist(normal_probs, bins=30, alpha=0.7, label='Normal', color='blue', density=True)\n",
        "plt.hist(stroke_probs, bins=30, alpha=0.7, label='Stroke', color='red', density=True)\n",
        "plt.xlabel('Predicted Stroke Probability')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Prediction Probability Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"ðŸ“ˆ Curve Metrics:\")\n",
        "print(f\"   ROC AUC: {roc_auc:.4f}\")\n",
        "print(f\"   Average Precision: {ap_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "ehtgM86tk8kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# SECTION 12: GRAD-CAM VISUALIZATION\n",
        "# ==========================================\n",
        "\n",
        "# Import Grad-CAM (separate cell to handle import issues)\n",
        "try:\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    # Install grad-cam if not available\n",
        "    try:\n",
        "        from pytorch_grad_cam import GradCAM\n",
        "    except ImportError:\n",
        "        print(\"ðŸ“¦ Installing pytorch-grad-cam...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"grad-cam\"])\n",
        "        from pytorch_grad_cam import GradCAM\n",
        "\n",
        "    from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "    from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "    gradcam_available = True\n",
        "    print(\"âœ… Grad-CAM imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"âš ï¸  Grad-CAM not available: {e}. Skipping visualizations.\")\n",
        "    gradcam_available = False\n",
        "\n",
        "if gradcam_available:\n",
        "    def get_vit_attention_map(model, input_tensor, target_layer_name='vit'):\n",
        "        \"\"\"Extract attention map from Vision Transformer\"\"\"\n",
        "        model.eval()\n",
        "\n",
        "        # Hook to capture attention weights\n",
        "        attention_weights = []\n",
        "\n",
        "        def attention_hook(module, input, output):\n",
        "            # For ViT, we want the attention weights from the last layer\n",
        "            if hasattr(module, 'attn') and hasattr(module.attn, 'attention_weights'):\n",
        "                attention_weights.append(module.attn.attention_weights)\n",
        "\n",
        "        # Register hook on the last transformer block\n",
        "        hooks = []\n",
        "        for name, module in model.vit.named_modules():\n",
        "            if 'blocks' in name and 'attn' in name:\n",
        "                hooks.append(module.register_forward_hook(attention_hook))\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            _ = model(input_tensor)\n",
        "\n",
        "        # Clean up hooks\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "\n",
        "        # If we got attention weights, process them\n",
        "        if attention_weights:\n",
        "            # Take the last layer's attention\n",
        "            attn = attention_weights[-1]  # [batch, heads, tokens, tokens]\n",
        "\n",
        "            # Average over heads and take CLS token attention to all patches\n",
        "            attn = attn.mean(dim=1)[0, 0, 1:].detach().cpu().numpy()  # Remove CLS token\n",
        "\n",
        "            # Reshape to spatial dimensions (14x14 for patch16 on 224x224)\n",
        "            grid_size = int(np.sqrt(len(attn)))\n",
        "            attn_map = attn.reshape(grid_size, grid_size)\n",
        "\n",
        "            return attn_map\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def create_hybrid_gradcam_visualization(model, dataset, num_samples=8):\n",
        "        \"\"\"Create comprehensive visualization showing ResNet, ViT, and combined results\"\"\"\n",
        "\n",
        "        # Set up Grad-CAM for ResNet component\n",
        "        resnet_target_layer = model.resnet.layer4[-1]\n",
        "        resnet_cam = GradCAM(model=model, target_layers=[resnet_target_layer])\n",
        "\n",
        "        # Select diverse samples\n",
        "        stroke_indices = [i for i, label in enumerate(dataset.labels) if label == 1]\n",
        "        normal_indices = [i for i, label in enumerate(dataset.labels) if label == 0]\n",
        "\n",
        "        # Mix samples from both classes\n",
        "        sample_indices = []\n",
        "        if len(stroke_indices) >= num_samples//2:\n",
        "            sample_indices.extend(np.random.choice(stroke_indices, num_samples//2, replace=False))\n",
        "        else:\n",
        "            sample_indices.extend(stroke_indices)\n",
        "\n",
        "        remaining = num_samples - len(sample_indices)\n",
        "        if len(normal_indices) >= remaining:\n",
        "            sample_indices.extend(np.random.choice(normal_indices, remaining, replace=False))\n",
        "        else:\n",
        "            sample_indices.extend(normal_indices)\n",
        "\n",
        "        sample_indices = sample_indices[:num_samples]\n",
        "\n",
        "        print(f\"ðŸŽ¨ Generating Enhanced Grad-CAM for {len(sample_indices)} samples...\")\n",
        "        print(\"   - ResNet feature maps\")\n",
        "        print(\"   - Vision Transformer attention\")\n",
        "        print(\"   - Combined visualization\")\n",
        "\n",
        "        # Create figure with subplots for each component\n",
        "        cols = 4  # Original, ResNet CAM, ViT Attention, Combined\n",
        "        rows = len(sample_indices)\n",
        "\n",
        "        plt.figure(figsize=(20, 5 * rows))\n",
        "\n",
        "        for idx, sample_idx in enumerate(sample_indices):\n",
        "            # Get sample info\n",
        "            img_path = dataset.paths[sample_idx]\n",
        "            true_label = dataset.labels[sample_idx]\n",
        "\n",
        "            # Get predictions if available\n",
        "            if sample_idx < len(all_logits):\n",
        "                pred_prob = all_logits[sample_idx]\n",
        "                pred_label = all_preds[sample_idx]\n",
        "            else:\n",
        "                # Make prediction for this sample\n",
        "                model.eval()\n",
        "                raw_img = Image.open(img_path).convert(\"RGB\")\n",
        "                transformed_img = val_tf(raw_img).unsqueeze(0).to(device)\n",
        "                with torch.no_grad():\n",
        "                    logits = model(transformed_img)\n",
        "                    pred_prob = logits.softmax(1)[0, 1].item()\n",
        "                    pred_label = logits.argmax(dim=1)[0].item()\n",
        "\n",
        "            # Load and preprocess image\n",
        "            raw_img = Image.open(img_path).convert(\"RGB\")\n",
        "            img_np = np.array(raw_img.resize((IMG_SIZE, IMG_SIZE))).astype(np.float32) / 255.0\n",
        "            transformed_img = val_tf(raw_img).unsqueeze(0).to(device)\n",
        "\n",
        "            # 1. Original Image\n",
        "            plt.subplot(rows, cols, idx * cols + 1)\n",
        "            plt.imshow(img_np)\n",
        "            plt.axis('off')\n",
        "            plt.title(f\"Original\\n{os.path.basename(img_path)[:15]}...\", fontsize=10)\n",
        "\n",
        "            # 2. ResNet Grad-CAM\n",
        "            targets = [ClassifierOutputTarget(1)]  # Target stroke class\n",
        "            resnet_cam_map = resnet_cam(input_tensor=transformed_img, targets=targets)\n",
        "            resnet_cam_map = resnet_cam_map[0, :]\n",
        "\n",
        "            plt.subplot(rows, cols, idx * cols + 2)\n",
        "            resnet_visualization = show_cam_on_image(img_np, resnet_cam_map, use_rgb=True)\n",
        "            plt.imshow(resnet_visualization)\n",
        "            plt.axis('off')\n",
        "            plt.title(\"ResNet Features\", fontsize=10)\n",
        "\n",
        "            # 3. Vision Transformer Attention\n",
        "            plt.subplot(rows, cols, idx * cols + 3)\n",
        "\n",
        "            # Try to get ViT attention map\n",
        "            vit_attention = get_vit_attention_map(model, transformed_img)\n",
        "\n",
        "            if vit_attention is not None:\n",
        "                # Resize attention map to image size\n",
        "                from scipy.ndimage import zoom\n",
        "                attention_resized = zoom(vit_attention,\n",
        "                                       (IMG_SIZE / vit_attention.shape[0],\n",
        "                                        IMG_SIZE / vit_attention.shape[1]))\n",
        "\n",
        "                # Normalize attention map\n",
        "                attention_resized = (attention_resized - attention_resized.min()) / \\\n",
        "                                  (attention_resized.max() - attention_resized.min())\n",
        "\n",
        "                # Create visualization\n",
        "                vit_visualization = show_cam_on_image(img_np, attention_resized, use_rgb=True)\n",
        "                plt.imshow(vit_visualization)\n",
        "                plt.title(\"ViT Attention\", fontsize=10)\n",
        "            else:\n",
        "                # Fallback: show a simple attention-style visualization\n",
        "                # Use gradient-based approach for ViT component\n",
        "                model.eval()\n",
        "                transformed_img.requires_grad_(True)\n",
        "\n",
        "                # Forward pass and get gradients\n",
        "                logits = model(transformed_img)\n",
        "                score = logits[0, 1]  # Stroke class score\n",
        "                score.backward()\n",
        "\n",
        "                # Get gradients\n",
        "                gradients = transformed_img.grad.data[0]\n",
        "\n",
        "                # Create simple saliency map\n",
        "                saliency = gradients.abs().mean(dim=0).cpu().numpy()\n",
        "                saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min())\n",
        "\n",
        "                vit_visualization = show_cam_on_image(img_np, saliency, use_rgb=True)\n",
        "                plt.imshow(vit_visualization)\n",
        "                plt.title(\"ViT Gradients\", fontsize=10)\n",
        "\n",
        "                # Clear gradients\n",
        "                transformed_img.grad = None\n",
        "\n",
        "            plt.axis('off')\n",
        "\n",
        "            # 4. Combined Visualization\n",
        "            plt.subplot(rows, cols, idx * cols + 4)\n",
        "\n",
        "            # Combine ResNet CAM with ViT attention\n",
        "            if vit_attention is not None:\n",
        "                combined_map = 0.6 * resnet_cam_map + 0.4 * attention_resized\n",
        "            else:\n",
        "                combined_map = 0.7 * resnet_cam_map + 0.3 * saliency\n",
        "\n",
        "            # Normalize combined map\n",
        "            combined_map = (combined_map - combined_map.min()) / \\\n",
        "                          (combined_map.max() - combined_map.min())\n",
        "\n",
        "            combined_visualization = show_cam_on_image(img_np, combined_map, use_rgb=True)\n",
        "            plt.imshow(combined_visualization)\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Create detailed title for combined view\n",
        "            true_class = \"Stroke\" if true_label == 1 else \"Normal\"\n",
        "            pred_class = \"Stroke\" if pred_label == 1 else \"Normal\"\n",
        "            confidence = pred_prob if pred_label == 1 else (1 - pred_prob)\n",
        "\n",
        "            title = f\"Combined View\\nTrue: {true_class}\\nPred: {pred_class} ({confidence:.3f})\"\n",
        "            plt.title(title, fontsize=10)\n",
        "\n",
        "        plt.suptitle('Enhanced Grad-CAM: ResNet + Vision Transformer Analysis', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def create_attention_summary(model, dataset, num_samples=4):\n",
        "        \"\"\"Create a summary view showing how different components contribute\"\"\"\n",
        "\n",
        "        print(f\"\\nðŸ” Creating Component Analysis Summary...\")\n",
        "\n",
        "        # Select samples (2 stroke, 2 normal)\n",
        "        stroke_indices = [i for i, label in enumerate(dataset.labels) if label == 1]\n",
        "        normal_indices = [i for i, label in enumerate(dataset.labels) if label == 0]\n",
        "\n",
        "        selected_indices = []\n",
        "        if len(stroke_indices) >= 2:\n",
        "            selected_indices.extend(np.random.choice(stroke_indices, 2, replace=False))\n",
        "        if len(normal_indices) >= 2:\n",
        "            selected_indices.extend(np.random.choice(normal_indices, 2, replace=False))\n",
        "\n",
        "        selected_indices = selected_indices[:num_samples]\n",
        "\n",
        "        fig, axes = plt.subplots(num_samples, 5, figsize=(25, 6 * num_samples))\n",
        "        if num_samples == 1:\n",
        "            axes = axes.reshape(1, -1)\n",
        "\n",
        "        for idx, sample_idx in enumerate(selected_indices):\n",
        "            img_path = dataset.paths[sample_idx]\n",
        "            true_label = dataset.labels[sample_idx]\n",
        "\n",
        "            # Load image\n",
        "            raw_img = Image.open(img_path).convert(\"RGB\")\n",
        "            img_np = np.array(raw_img.resize((IMG_SIZE, IMG_SIZE))).astype(np.float32) / 255.0\n",
        "            transformed_img = val_tf(raw_img).unsqueeze(0).to(device)\n",
        "\n",
        "            # Get model prediction and feature analysis\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                # Forward pass through components\n",
        "                resnet_features = model.pool(model.resnet.forward_features(transformed_img)).flatten(1)\n",
        "                vit_features = model.vit.forward_features(transformed_img)\n",
        "                if vit_features.ndim == 3:\n",
        "                    vit_features = vit_features[:, 0, :]\n",
        "\n",
        "                # Combined features\n",
        "                combined_features = torch.cat([resnet_features, vit_features], dim=1)\n",
        "                final_logits = model.classifier(combined_features)\n",
        "                final_prob = final_logits.softmax(1)[0, 1].item()\n",
        "\n",
        "            # Original image\n",
        "            axes[idx, 0].imshow(img_np)\n",
        "            axes[idx, 0].set_title(f\"Original\\n{os.path.basename(img_path)[:20]}\", fontsize=10)\n",
        "            axes[idx, 0].axis('off')\n",
        "\n",
        "            # ResNet CAM\n",
        "            resnet_cam = GradCAM(model=model, target_layers=[model.resnet.layer4[-1]])\n",
        "            targets = [ClassifierOutputTarget(1)]\n",
        "            resnet_map = resnet_cam(input_tensor=transformed_img, targets=targets)[0]\n",
        "\n",
        "            resnet_vis = show_cam_on_image(img_np, resnet_map, use_rgb=True)\n",
        "            axes[idx, 1].imshow(resnet_vis)\n",
        "            axes[idx, 1].set_title(\"ResNet Focus\", fontsize=10)\n",
        "            axes[idx, 1].axis('off')\n",
        "\n",
        "            # ViT attention/gradients\n",
        "            vit_attention = get_vit_attention_map(model, transformed_img)\n",
        "            if vit_attention is not None:\n",
        "                from scipy.ndimage import zoom\n",
        "                attention_resized = zoom(vit_attention,\n",
        "                                       (IMG_SIZE / vit_attention.shape[0],\n",
        "                                        IMG_SIZE / vit_attention.shape[1]))\n",
        "                attention_resized = (attention_resized - attention_resized.min()) / \\\n",
        "                                  (attention_resized.max() - attention_resized.min())\n",
        "                vit_vis = show_cam_on_image(img_np, attention_resized, use_rgb=True)\n",
        "                title = \"ViT Attention\"\n",
        "            else:\n",
        "                # Fallback to gradients\n",
        "                transformed_img.requires_grad_(True)\n",
        "                logits = model(transformed_img)\n",
        "                score = logits[0, 1]\n",
        "                score.backward()\n",
        "                gradients = transformed_img.grad.data[0]\n",
        "                saliency = gradients.abs().mean(dim=0).cpu().numpy()\n",
        "                saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min())\n",
        "                vit_vis = show_cam_on_image(img_np, saliency, use_rgb=True)\n",
        "                title = \"ViT Gradients\"\n",
        "                transformed_img.grad = None\n",
        "\n",
        "            axes[idx, 2].imshow(vit_vis)\n",
        "            axes[idx, 2].set_title(title, fontsize=10)\n",
        "            axes[idx, 2].axis('off')\n",
        "\n",
        "            # Combined view\n",
        "            if vit_attention is not None:\n",
        "                combined_map = 0.6 * resnet_map + 0.4 * attention_resized\n",
        "            else:\n",
        "                combined_map = 0.7 * resnet_map + 0.3 * saliency\n",
        "\n",
        "            combined_map = (combined_map - combined_map.min()) / \\\n",
        "                          (combined_map.max() - combined_map.min())\n",
        "            combined_vis = show_cam_on_image(img_np, combined_map, use_rgb=True)\n",
        "\n",
        "            axes[idx, 3].imshow(combined_vis)\n",
        "            axes[idx, 3].set_title(\"Combined\", fontsize=10)\n",
        "            axes[idx, 3].axis('off')\n",
        "\n",
        "            # Prediction summary\n",
        "            axes[idx, 4].text(0.1, 0.8, f\"True Label: {'Stroke' if true_label == 1 else 'Normal'}\",\n",
        "                             fontsize=12, transform=axes[idx, 4].transAxes)\n",
        "            axes[idx, 4].text(0.1, 0.6, f\"Prediction: {'Stroke' if final_prob > 0.5 else 'Normal'}\",\n",
        "                             fontsize=12, transform=axes[idx, 4].transAxes)\n",
        "            axes[idx, 4].text(0.1, 0.4, f\"Confidence: {final_prob:.3f}\",\n",
        "                             fontsize=12, transform=axes[idx, 4].transAxes)\n",
        "\n",
        "            # Feature contribution analysis\n",
        "            resnet_norm = torch.norm(resnet_features).item()\n",
        "            vit_norm = torch.norm(vit_features).item()\n",
        "            total_norm = resnet_norm + vit_norm\n",
        "\n",
        "            axes[idx, 4].text(0.1, 0.2, f\"ResNet contrib: {resnet_norm/total_norm:.2f}\",\n",
        "                             fontsize=10, transform=axes[idx, 4].transAxes)\n",
        "            axes[idx, 4].text(0.1, 0.1, f\"ViT contrib: {vit_norm/total_norm:.2f}\",\n",
        "                             fontsize=10, transform=axes[idx, 4].transAxes)\n",
        "\n",
        "            axes[idx, 4].set_xlim(0, 1)\n",
        "            axes[idx, 4].set_ylim(0, 1)\n",
        "            axes[idx, 4].axis('off')\n",
        "            axes[idx, 4].set_title(\"Analysis\", fontsize=10)\n",
        "\n",
        "        plt.suptitle('Component Analysis: ResNet vs Vision Transformer Contributions', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ðŸŽ¨ ENHANCED GRAD-CAM VISUALIZATIONS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Generate comprehensive visualizations\n",
        "    print(\"ðŸ” Generating hybrid ResNet-ViT visualizations...\")\n",
        "    create_hybrid_gradcam_visualization(model, val_ds, num_samples=6)\n",
        "\n",
        "    print(\"\\nðŸ”¬ Generating component analysis summary...\")\n",
        "    create_attention_summary(model, val_ds, num_samples=4)\n",
        "\n",
        "    print(\"âœ… Enhanced Grad-CAM analysis completed!\")\n",
        "\n",
        "else:\n",
        "    print(\"âš ï¸ Skipping Grad-CAM visualizations due to import issues.\")\n",
        "    print(\"ðŸ’¡ To manually install: !pip install grad-cam\")"
      ],
      "metadata": {
        "id": "lOXS_dMBlBCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# SECTION 13: ANALYSIS BY IMAGE TYPE\n",
        "# ==========================================\n",
        "\n",
        "def analyze_by_modality(image_paths, predictions, targets, probabilities):\n",
        "    \"\"\"Analyze performance by imaging modality (BT vs MR)\"\"\"\n",
        "\n",
        "    bt_indices = [i for i, path in enumerate(image_paths)\n",
        "                  if any(x in path.upper() for x in ['BT', 'BRAIN', 'TOMOGRAPHY'])]\n",
        "    mr_indices = [i for i, path in enumerate(image_paths)\n",
        "                  if any(x in path.upper() for x in ['MR', 'MAGNETIC', 'RESONANCE'])]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ðŸ”¬ ANALYSIS BY IMAGING MODALITY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    modalities = [\n",
        "        (\"Brain Tomography (BT)\", bt_indices),\n",
        "        (\"Magnetic Resonance (MR)\", mr_indices)\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for name, indices in modalities:\n",
        "        if len(indices) == 0:\n",
        "            continue\n",
        "\n",
        "        subset_preds = np.array([predictions[i] for i in indices])\n",
        "        subset_targets = np.array([targets[i] for i in indices])\n",
        "        subset_probs = np.array([probabilities[i] for i in indices])\n",
        "\n",
        "        accuracy = accuracy_score(subset_targets, subset_preds)\n",
        "\n",
        "        # Calculate AUROC if both classes are present\n",
        "        unique_targets = np.unique(subset_targets)\n",
        "        if len(unique_targets) > 1:\n",
        "            fpr_sub, tpr_sub, _ = roc_curve(subset_targets, subset_probs)\n",
        "            subset_auc = auc(fpr_sub, tpr_sub)\n",
        "            subset_ap = average_precision_score(subset_targets, subset_probs)\n",
        "        else:\n",
        "            subset_auc = \"N/A\"\n",
        "            subset_ap = \"N/A\"\n",
        "\n",
        "        print(f\"\\nðŸ“Š {name} Images:\")\n",
        "        print(f\"   Sample Count: {len(indices)}\")\n",
        "        print(f\"   Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"   AUROC: {subset_auc}\")\n",
        "        print(f\"   Average Precision: {subset_ap}\")\n",
        "\n",
        "        if len(unique_targets) > 1:\n",
        "            print(f\"   Classification Breakdown:\")\n",
        "            report = classification_report(subset_targets, subset_preds,\n",
        "                                         target_names=['Normal', 'Stroke'],\n",
        "                                         output_dict=True)\n",
        "            for class_name in ['Normal', 'Stroke']:\n",
        "                if class_name.lower() in report:\n",
        "                    metrics = report[class_name.lower()]\n",
        "                    print(f\"     {class_name}: Precision={metrics['precision']:.3f}, \"\n",
        "                          f\"Recall={metrics['recall']:.3f}, F1={metrics['f1-score']:.3f}\")\n",
        "\n",
        "        results.append({\n",
        "            'modality': name,\n",
        "            'count': len(indices),\n",
        "            'accuracy': accuracy,\n",
        "            'auroc': subset_auc if isinstance(subset_auc, str) else f\"{subset_auc:.4f}\",\n",
        "            'ap': subset_ap if isinstance(subset_ap, str) else f\"{subset_ap:.4f}\"\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Perform modality analysis\n",
        "modality_results = analyze_by_modality(all_image_paths, all_preds, all_targets, all_logits)\n"
      ],
      "metadata": {
        "id": "ikH0AyL_lDco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# SECTION 14: SAVE COMPREHENSIVE RESULTS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ’¾ SAVING COMPREHENSIVE RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create detailed results DataFrame\n",
        "results_data = {\n",
        "    'image_path': all_image_paths,\n",
        "    'filename': [os.path.basename(p) for p in all_image_paths],\n",
        "    'true_label': all_targets,\n",
        "    'predicted_label': all_preds,\n",
        "    'stroke_probability': all_logits,\n",
        "    'correct_prediction': (all_targets == all_preds).astype(int),\n",
        "    'confidence': np.where(all_preds == 1, all_logits, 1 - all_logits)\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results_data)\n",
        "\n",
        "# Add derived columns\n",
        "results_df['image_type'] = results_df['image_path'].apply(\n",
        "    lambda x: 'BT' if any(term in x.upper() for term in ['BT', 'BRAIN', 'TOMOGRAPHY'])\n",
        "             else 'MR' if any(term in x.upper() for term in ['MR', 'MAGNETIC', 'RESONANCE'])\n",
        "             else 'Unknown'\n",
        ")\n",
        "\n",
        "results_df['true_class'] = results_df['true_label'].map({0: 'Normal', 1: 'Stroke'})\n",
        "results_df['predicted_class'] = results_df['predicted_label'].map({0: 'Normal', 1: 'Stroke'})\n",
        "\n",
        "# Save main results\n",
        "results_df.to_csv('stroke_classification_results.csv', index=False)\n",
        "print(\"âœ… Detailed results saved to 'stroke_classification_results.csv'\")\n",
        "\n",
        "# Create summary statistics\n",
        "summary_stats = {\n",
        "    'metric': ['Total Images', 'Overall Accuracy', 'AUROC', 'Average Precision',\n",
        "               'True Positives', 'True Negatives', 'False Positives', 'False Negatives'],\n",
        "    'value': [len(results_df), f\"{overall_accuracy:.4f}\", f\"{roc_auc:.4f}\", f\"{ap_score:.4f}\",\n",
        "              int(tp), int(tn), int(fp), int(fn)]\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(summary_stats)\n",
        "summary_df.to_csv('summary_statistics.csv', index=False)\n",
        "print(\"âœ… Summary statistics saved to 'summary_statistics.csv'\")\n"
      ],
      "metadata": {
        "id": "fDuCGDpolFaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# SECTION 15: FINAL SUMMARY REPORT\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“‹ FINAL COMPREHENSIVE REPORT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\"\"\n",
        "ðŸŽ¯ MODEL PERFORMANCE SUMMARY:\n",
        "   â€¢ Total Validation Images: {len(results_df):,}\n",
        "   â€¢ Overall Accuracy: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\n",
        "   â€¢ AUROC Score: {roc_auc:.4f}\n",
        "   â€¢ Average Precision: {ap_score:.4f}\n",
        "\n",
        "ðŸ“Š CONFUSION MATRIX:\n",
        "   â€¢ True Positives (Stroke correctly identified): {tp}\n",
        "   â€¢ True Negatives (Normal correctly identified): {tn}\n",
        "   â€¢ False Positives (Normal misclassified as Stroke): {fp}\n",
        "   â€¢ False Negatives (Stroke misclassified as Normal): {fn}\n",
        "\n",
        "ðŸ“ˆ CLASS-WISE PERFORMANCE:\n",
        "\"\"\")\n",
        "\n",
        "class_breakdown = results_df.groupby('true_class').agg({\n",
        "    'correct_prediction': ['count', 'sum', 'mean'],\n",
        "    'confidence': 'mean'\n",
        "}).round(4)\n",
        "\n",
        "for true_class in ['Normal', 'Stroke']:\n",
        "    if true_class in class_breakdown.index:\n",
        "        count = class_breakdown.loc[true_class, ('correct_prediction', 'count')]\n",
        "        correct = class_breakdown.loc[true_class, ('correct_prediction', 'sum')]\n",
        "        accuracy = class_breakdown.loc[true_class, ('correct_prediction', 'mean')]\n",
        "        avg_conf = class_breakdown.loc[true_class, ('confidence', 'mean')]\n",
        "\n",
        "        print(f\"   â€¢ {true_class}: {correct}/{count} correct ({accuracy:.4f} accuracy)\")\n",
        "        print(f\"     Average Confidence: {avg_conf:.4f}\")\n",
        "\n",
        "print(f\"\"\"\n",
        "ðŸ”¬ MODALITY BREAKDOWN:\n",
        "\"\"\")\n",
        "\n",
        "modality_breakdown = results_df.groupby('image_type').agg({\n",
        "    'correct_prediction': ['count', 'mean'],\n",
        "    'confidence': 'mean'\n",
        "}).round(4)\n",
        "\n",
        "for img_type in modality_breakdown.index:\n",
        "    count = modality_breakdown.loc[img_type, ('correct_prediction', 'count')]\n",
        "    accuracy = modality_breakdown.loc[img_type, ('correct_prediction', 'mean')]\n",
        "    avg_conf = modality_breakdown.loc[img_type, ('confidence', 'mean')]\n",
        "\n",
        "    print(f\"   â€¢ {img_type}: {count} images, {accuracy:.4f} accuracy, {avg_conf:.4f} avg confidence\")\n",
        "\n",
        "print(f\"\"\"\n",
        "ðŸ’¾ SAVED FILES:\n",
        "   â€¢ stroke_classification_results.csv - Detailed per-image results\n",
        "   â€¢ summary_statistics.csv - Overall performance metrics\n",
        "   â€¢ model_epoch_*.pth - Model checkpoints\n",
        "\n",
        "âœ… COMPLETE DATASET PROCESSING FINISHED!\n",
        "\"\"\")\n",
        "\n",
        "print(\"ðŸŽ‰ All sections completed successfully!\")"
      ],
      "metadata": {
        "id": "YTXZQfy4j3Tg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}